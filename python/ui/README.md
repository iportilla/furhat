Here is a clean, professional, copy/pasteâ€“ready README.md for your entire Furhat + Ollama streaming robot project.

â¸»

â­ README.md â€” Furhat + Ollama Streaming Conversational Robot

Real-time LLM Streaming â†’ Real-time Robot Speech

â¸»

ğŸš€ Overview

This project connects a Furhat social robot with an Ollama LLM server to create a natural, real-time streaming conversational robot.

It enables:
	â€¢	ğŸ¤ Furhat listens to the user
	â€¢	ğŸ§  Ollama generates a response in streaming mode (token-by-token)
	â€¢	ğŸ—£ï¸ Furhat speaks the response as it is generated
	â€¢	ğŸ“œ Streamlit UI shows live conversation transcript
	â€¢	âš™ï¸ User can configure all connection parameters from UI
	â€¢	ğŸ”„ Robust automatic looping for continuous conversation

This system is optimized for minimal latency, smooth turn-taking, and real-world robot interaction.

â¸»

ğŸ“ Folder Structure

furhat-python/
â”‚
â””â”€â”€ ui/
    â”œâ”€â”€ streaming_ui.py                 # Streamlit UI to configure and launch the robot
    â”œâ”€â”€ furhat_ollama_streamchat.py     # Main streaming robot loop
    â”œâ”€â”€ conversation_log.jsonl          # Auto-generated live conversation transcript
    â””â”€â”€ README.md                       # (this file)


â¸»

ğŸ§© Components

1. streaming_ui.py

A Streamlit dashboard that:
	â€¢	Lets user configure:
	â€¢	Furhat IP address
	â€¢	Ollama IP / hostname
	â€¢	LLM model (e.g., llama3.2:3b)
	â€¢	System prompt / personality
	â€¢	Launches the robot backend as a background process
	â€¢	Displays live conversation in real time (auto-refresh)

â¸»

2. furhat_ollama_streamchat.py

The core system:
	â€¢	Connects to Furhat via Furhat Realtime API
	â€¢	Listens for user utterances (response_hear_end)
	â€¢	Calls Ollama via /api/chat with stream=True
	â€¢	Streams token chunks and buffers partial speech
	â€¢	Furhat speaks incrementally as tokens arrive
	â€¢	Logs full conversation to conversation_log.jsonl

This results in highly natural â€œrobot thinking aloudâ€ behavior.

â¸»

ğŸ› ï¸ Requirements

Install Python dependencies:

pip install streamlit httpx streamlit-autorefresh furhat-realtime-api

Also install Ollama:

macOS or Linux:

brew install ollama
ollama serve

Or download from:
https://ollama.com

Optional: Pull a model:

ollama pull llama3.2:3b


â¸»

ğŸ”§ Configuration

1ï¸âƒ£ Furhat Robot Requirements

Your robot must:
	â€¢	Be on the same network as your machine
	â€¢	Be reachable on the Realtime API port
	â€¢	Support ASR (Automatic Speech Recognition)

Test connection:

from furhat_realtime_api import AsyncFurhatClient
import asyncio

async def test():
    f = AsyncFurhatClient("172.27.8.18")
    await f.connect()
    await f.request_speak_text("Hello, I am online.")
    await f.disconnect()

asyncio.run(test())


â¸»

2ï¸âƒ£ Ollama Requirements

Run Ollama server:

ollama serve

Test:

curl http://127.0.0.1:11434/api/tags

Remote IPs are fully supported (just allow port 11434).

â¸»

â–¶ï¸ How to Run the System

Step 1 â€” Start Ollama Server

On your local machine or remote server:

ollama serve

(Optional) Pull a model:

ollama pull llama3.2:3b


â¸»

Step 2 â€” Launch Streamlit UI

Navigate to the ui/ folder:

cd furhat-python/ui
streamlit run streaming_ui.py

This opens a browser window with:
	â€¢	Furhat IP input
	â€¢	Ollama server IP input
	â€¢	Model selection
	â€¢	System prompt editor
	â€¢	Live conversation log

â¸»

Step 3 â€” Start Robot Conversation
	1.	Fill out the settings
	2.	Press â€œStart Robot Conversationâ€

The Streamlit UI launches:

furhat_ollama_streamchat.py

This script handles all robot interaction.

â¸»

ğŸ¤ What Happens During Conversation
	1.	Furhat listens
	2.	When the user stops speaking â†’ event triggers
	3.	Text is sent to Ollama in streaming mode
	4.	LLM sends tokens like:

"Hello"
", I"
" am"
" a robot"
"..."

	5.	Furhat receives these partial chunks
	6.	Furhat speaks them immediately
	7.	The UI logs both user & robot messages

This results in almost no delay, very human-like interaction.

â¸»

ğŸ“œ Logs & Monitoring

All conversation turns (user + robot) are stored as JSON lines:

conversation_log.jsonl

Example:

{"role": "user", "text": "Tell me about space", "timestamp": 1731879192.2}
{"role": "assistant", "text": "Space is huge...", "timestamp": 1731879193.1}

The Streamlit UI displays this in real time.

â¸»

ğŸ§ª Troubleshooting

âŒ Furhat does not speak
	â€¢	Check robot IP
	â€¢	Confirm Realtime API port accessibility
	â€¢	Ensure your network doesnâ€™t block UDP or WebSocket traffic

âŒ Ollama not reachable

Test:

curl http://YOUR_IP:11434/api/tags

âŒ UI not updating

Ensure you installed:

pip install streamlit-autorefresh


ğŸ’» VSCode devcontainer

ğŸ§ª Unit tests for streaming

Just ask!
